I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

-----------------------------------------------------------------------------
This Branch Is About Integrating The hamsterdb2 Functionality!!!!!
-----------------------------------------------------------------------------
The big headline is:
As a user i want to run many Transactions in parallel with high performance.
I'm using multiple threads b/c my CPU has multiple cores, and expect hamsterdb
to scale with the number of cores.
==============================================================================

high-level plan for 2.1.8 pro ..............................................
x all changes of APL version
x SIMD for default layout/PAX keylists
x CRC32
x SIMD for real64
x add parameter to limit file size (APL + PRO)
x benchmark (APL)
o bitmap index
. grouped varints

x adapt to the newest changes from 2.1.8
    x move global variables to Globals
    x verify that aes encryption still works
    x verify that key compression is still working for normal keys
    x verify that key compression is still working for extkeys
    x verify that record compression is still working
    x verify that journal compression is still working
    x verify that SSE is still working for PAX layouts

x PRO: enable SIMD in the default layout if it uses the same KeyList as
    in PAX *and* if record IDs are stored separately from the keys

x PRO: CRC32-checksums (as soon as the file format is updated)
    x SSE2 has support for a CRC32 checksum calculation, or find a good
        library -> use murmurhash3
    x needs new flag HAM_ENABLE_CRC32
    x document the flag
    x do not allow with in-memory Environments
        x unittest
    x flag is not persisted
        x unittest
    x write in Page::flush (if page has a header), verify in Page::read
        (if page has a header)
    x multipage blobs: store in PBlobPageHeader::m_free_bytes
    x add to ham_bench
    x add unittest
        x corrupt a page
        x allocate, read, overwrite, erase etc multi-page blobs
        x corrupt a multi-page blob
    x add to monster-pro tests (incl. reopen, big blobs, overwrite)

    x apl: support and document the flag, but return error if selected
        x add to java api
        x add to .NET api
        x add to python api
        x add to erlang api

x PRO: add sse support for real64
    __cmpeq_pd
    everything else is same as with uint64?

x add parameter to limit file size (also for APL)
    x make sure that db does not end up in an inconsistent state if a
        page allocation fails!
    x device_disk.h: cache the file size (but verify it in debug builds)
    x add parameter and documentation (header file)
        HAM_PARAM_FILE_SIZE: 0x00000109
    x fail if in-memory db
        x unittest
    x device_disk.h: check for the file size limit (default: 0xffffffffff...)
    x add unittest
        x check integrity after a split
        x when allocating a blob: make sure that the inserted key is removed
            afterwards (check integrity)
    x add to java
    x add to .NET
    x add to python
    x add to erlang

x hola: extend the documentation

x collect/publish benchmark against leveldb (with and without SSD)
    x uint64, small records, random write
    x uint64, small records, random read
    x uint64, small records, linear write
    x uint64, small records, linear read
    x uint64, large records, random write
    x uint64, large records, random read
    x uint64, large records, linear write
    x uint64, large records, linear read
    x bin16, default records, random write (w/o compression)
    x bin16, default records, linear write (w/o compression)
    x bin16, default records, random read (w/o compression)
    x bin16, default records, linear read (w/o compression)
    x uint64, calculate sum()
    x bin16, use count_if() of all keys with a certain postfix ("X")

    x make sure that the hamsterdb databases are using the correct parameters
        x bin16 should use fixed length keys

    x SSD: run each with 100k, 1m, 10m and 100m keys (4 mb cache)
    x SSD: run each with 100k, 1m, 10m and 100m keys (1 gb cache)
    x HDD: run each with 100k, 1m, 10m and 100m keys (4 mb cache)
    x HDD: run each with 100k, 1m, 10m and 100m keys (1 gb cache)

x make sure that hamsterdb/master can compile against libuv 0.10.25
    x rebase v2 to master
    x rebase pro to base

x Refactoring: preparations for bitmap compression
    x btree_index_factory: RecordNumberCompare is no longer necessary since
        numbers are now host endian (it's a synonym for uint64!)
    x KeyList::has_simd_support() is the same as KeyList::kIsSimdEnabled
        remove the latter
    x move each KeyList and RecordList into its own file
        x also make sure that each file is properly documented
        x update Makefile.am
    x kHasSequentialData still required? - yes, absolutely
    x have both btree layouts derive from common subclass and reduce code
        x file is btree_impl_base.h
    x once more rewrite the capacity calculations in the LayoutImpl: right now
        it's based on byte-wise calculations, but these do not work with
        the bitmap (and with other sub-byte compression schemes as well)
        -> use double for key size calculations (get_full_key_size(),
            get_full_record_size())

x PRO: KeyList::get_key_data() is only required to retrieve a buffer
    for SIMD search; rename the function (i.e. "get_simd_data()")

o PRO: use a bitmap index for recnos
    -> can we come up with a solution that also works for non-recnos?
        then we need to implement sparse indices (maybe even compress them
        with RLEs)
    -> compression idea:
        a "BitVector" is a bitmap stored in 32bit
        2 bits per BitVector for flags
            00: BitVector contains all zeroes (-> macromap is not required)
            11: BitVector contains all ones (-> macromap is not required)
            10: BitVector contains a bitmap (-> macromap is required)
        a "minimap" is a set of flags (32bit -> 16 flags) with the corresponding
            (unknown) number of macromaps; minimum size is 4 bytes, 
            maximum is sizeof(T) + 17 * 4 = sizeof(T) + 68

    x implement a testable template version of a minimap
        template parameters: type T (uint16, uint32, uint64)
                number of 2bit flags
        x needs to shrink if all bits of a word are set (or not set)
        x must iterate; use precomputed lookup-table for the flag word
        x should be serialized in a flat buffer
        x must be able to insert/append, flip bits, test bits
        x sparsemap: add another layer on top of this which manages a variable
            number of minimaps, and can iterate, lookup and insert
            x all minimap offsets must be aligned to their capacity! otherwise
                they must get merged and this creates awful complexity
            x must be able to insert/append, test bits
            x use a fixed length memory block instead of std::vector
            x many unittests
                x set range(1..10000), test if bit is set
                x unset range(1..10000), test if bit is set
                x set range(10000..1) in reverse order
                x unset range(10000..1) in reverse order
                x set 1; set 2048*2+1; set 2048; must insert a MiniMap in
                    between two other MiniMaps
                x unset all above -> should we delete the full minimap?
                    implemented, but tests are failing
        x check for overflow when growing; make sure that the code flow
            is exception safe!
        x review/refactor/beautify code

    x add to header file
    x need new parameter/compression type; store it persistently
        x unittest
    x add to ham_bench
    x print in ham_info
    x only allowed for uint16, uint32 and uint64 keys

    x rebase to v2
        x verify that aes encryption still works
        x verify that journal compression is still working
        x verify that record compression is still working
        x verify that SSE is still working for PAX layouts
        x verify that key compression is still working for normal keys
        x verify that key compression is still working for extkeys

    x create a resizable KeyList based on a sparsemap
        x start with a minimal KeyList which compiles, but does not have any
            functionality
        x use KeyList only for leafs (b/c keys in the internal nodes are
            too sparse)
        x implement erase_slot()
        x implement insert() - cannot work on slots!!
        x search: use skip-list approach? binary search will be inefficient
            -> simply set m_keys.linear_search_threshold to 0
        x fix capacity calculation
            x initial key size: KeyList::get_full_key_size (return 1.0/8.0)
            x implement requires_split()
            x implement change_capacity()
        x implement print() and get_key()
            -> sparsemap.h: implement nth_set_bit()
        x for hola scans: don't use iterators but uncompress keys in a buffer
            (size depends on BitVector size) -> caller interface can stay as is
        x btree split: pivot interface must be chosen (or "adjusted") by the
            KeyList
        x use 32bit words instead of 64bit; otherwise the minimum capacity
            is too big for default-sized records 
            x also make sure that records are not inlined if the minimum
                capacity of the bitmap is missed
        x implement copy_to()
            assert that the slot is on a minimap-boundary and properly aligned,
            then simply copy the minimaps
        x fix remaining TODOs in btree_keys_bitmap.h

        x run some basic tests with ham_bench
            x make sure that flag is persisted and used
            x currently always performs very slow binary search

        x unittests
            x integrate sparsemap's unittests (w/ both word sizes!)
            x sparsemap-tests for move_and_append()
            x check capacity of node w/ default records
            x check capacity of node w/ empty records
            x insert/find/erase/find

    x there are currently several conceptual problems:
        1. the capacity of the node must be aligned to the bitmap capacity
            -> fix the initialization routine
        2. if duplicates are enabled then it's possible that the minimum
            capacity of a bitmap will not fit in a page 
        3. the "capacity"/aligned minimaps are problematic because the number
            of records will not exceed the minimap capacity. therefore there is
            lots of free space in the node. as long as the minimaps are
            aligned, this will be hard to solve

    x MiniMaps should remain aligned, but to the Wordsize and not to the
        full capacity. Unused words are flagged as 2#01, and cannot grow
        because then they might have to be merged with the following MiniMap.
        x adjust the lookup table (calc_vector_size)
        x adjust get_size()
        x introduce get_capacity()
        x fix everything else
    x fix the node initialization - not enough bytes are currently allocated
        -> is ok

    x SparseMap needs to know its capacity
    x SparseMap needs to know its *remaining* capacity

    x when creating a MiniMap: pass the remaining capacity; set the MiniMap
        flags in m_data[0] (can use a lookup table)
        x then update the remaining capacity in the SparseMap
    x update after move_and_append()

    x rename move_and_split() to split()
    x adjust the split function; it should be possible to split a SparseMap
        into multiple MiniMaps -> make sure that we can split word-aligned

    x make sure that the KeyList resizes properly
        -> is not ok yet - seems to resize too much, then the node
            ends up with 44 bytes left
        x make sure that resizing is word-aligned!
        x remove the capacity interface of the sparsemap - it's not required

    x if a minimap is inserted *between* two minimaps then it has to
        make sure that the capacities don't overlap
        x this affects splits as well? - no
        x merges? - no
        - if a minimap is appended, the start of the next sibling has to
            be checked
        - also when it's prepended
        -> we cannot solve this issue with the current design (and I do not
            want to change the design); but it would work for recnos!
        x make sure that with recnos existing keys can be deleted or
            overwritten, but not re-inserted!
        x ham_bench: support recnos (hamsterdb, berkeleydb)
        x allow bitmap compression with recnos only
        x splits/merges
             -> run insert/find/erase/find test, but with default record
                 sizes; this causes 7-8 splits and merges
        x cursor moves
        x hola
            x without transactions
            x with transactions

    o add many monster tests
        x different record sizes
            x --record-number --recsize=8 --key-compression=bitmap 
            x --record-number --recsize-fixed=1 --key-compression=bitmap 
        x without records
            x --record-number --recsize-fixed=0 --key-compression=bitmap 
        x with recovery && reopen
            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --use-recovery
            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --use-recovery --open --find-pct=100
        o with find & erase
            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1
            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --open --find-pct=100
=========================
here is a conceptual problem. By erasing many elements, the size of the
sparsemap will grow and overwrite the record area. in other words: before
erasing, the node has either to be split or rearranged.
-> move the split logic to btree_janitor.h/.cc
-> if bitmap compression is enabled then check if the keylist requires a split
    -> if yes then split
    -> not so trivial in btree_erase.cc. hard to say whether we would get more
        issues
    -> or bite the bullet and use a non-compressed bitmap?

            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --open --erase-pct=50 --find-pct=50
=========================
            x --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --open --find-pct=100
            o --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --open --erase-pct=90 --find-pct=10
            o --record-number --recsize-fixed=5 --key-compression=bitmap --seed=1 --open --find-pct=100
        o add all those tests to the monster suite

    o (APL) disable duplicates in combination with recnos

    o (APL) RecordNumbers always start with 1, which means bit 0 will always be
        false and the first bitmap will not be compressible. Make sure that
        internally, recno's start with 0

    o improve sparsemap performance
        suggestion: use fixed-length lookup table with offsets for the minimap
        the last entry is always for the last minimap (this will be the
        starting point for appending bits)
        x scan() has many branches because of the "skip" - can they be avoided?
        o perform a profiling run with valgrind
        o insert() checks if it can prepend/append, but this requires
            a call to sparsemap::select() - avoid if recnos are enabled
        o we can make select faster by checking if *p == 0xff
        o very fast code path for appending bits for recno's
        . get_capacity() is called frequently, but there's a difference between
            calculating capacity and comparing the index, or checking whether
            the index is *in* that minimap capacity; the second one is faster
        . fast random access (use lookup table of minimaps and
                their offsets)
        . some functions like get_key() (called in cursor_move) are slow
            because they operate on "slots" instead of keys. can they be
            optimized? (i.e. add popcount to the lookup table)

    o improve separation between hamsterdb sources and sparsemap by deriving
        from sparsemap::SparseMap, and use the derived class
        o move select(), count(), split() to the derived class
        o ... and everything else that is hamsterdb-related
    o document in header file (esp. for recno!)
    o document - write a paper etc to claim prior art; release code as a
        separate open-source library (GPL)?

o webpage changes
    o give away for free to non-profits
    o also add a startup program?
    o add CRC32 to pro documentation, feature matrix

------------- hamsterdb 2.1.9 ----------------------------------------

o refactoring: split files
    0root/root.h - very first header file (only one!)
    1base/error.h, pack*, util, mutex, version - all stuff based on 0root
    1os/os* - all stuff based on 1base (split in os_file.h and os_socket.h)
    1mem
    2page/
    2changeset/
    2device/
    2protoserde/
    2protobuf/
    3blob/
    3journal/
    3log/
    3page_manager/
    3btree/btree*
    4txn/...
    4db/...
    4env/...
    4cursor/...
    5server/...
    hamsterdb.cc, hola.cc (remain in root directory)
    o root0: add pragmas from btree_impl_default.h, undef min/max
    o #34: header guards/macros should not clash with reserved identifiers
        (no leading/trailing underscores)
    o enforce "layers" with macros in header file (the code shown here
        is problematic because it does not allow to include header files
        between start of the guard and the namespace declaration...)
    o enforce "layers" with macros in header file
        (HAM_HEADER_START(tier, guard)
            -> #ifndef guard
               #  define guard
               #if current_tier > tier
               #  error ...
               #  current_tier = tier
               #endif
               #if namespace_Started
               #  error ...
               #endif
               #define namespace_Started 1
               #ifndef root0_included
               #  error 0root/root.h was not included
               #endif
               namespace hamsterdb {
        HAM_HEADER_END()
               } // namespace hamsterdb
               #endif // guard
               #undef namespace_Started

o java still has license.java/version.java - remove!
    o also check dotnet!

o remove version.h, embed in hamsterdb.h

o refactoring: get_linear_search_threshold returns size_t, but is
    then used as (signed) int; some return 0, others -1
    right now the code works but is cheesy. return (int)0 to disable.

o refactoring: split unittests/cursor.cpp - it does not compile on smaller VMs

o refactoring: #37 - hamzilla.c/running should be sig_atomic_t
    -> translate to c++ code

o can we add a generic macro like this:
    #define ham_make_key(PTR, SIZE)   { SIZE, PTR, 0, 0}
    which can be used as an initializer:
    ham_key_t key = ham_make_key("hello", 5);
    o same for records
    o use in samples
    o use in source
    o use in unittests
    o document

o review exception safety
    http://exceptionsafecode.com/
    o consistently use smart_pointers to manage allocated resources!
        -> define rules
    o introduce on_scope_exit macro
    o consistently force use of RAII
    o every destructor has to clean up but MOST NOT THROW!
    o support std::swap
        -> define rules
    o manage state like a resource
        -> define rules
    o use "critical line" for strong guarantees and defer commits till
        success is guaranteed
    o every state has to be restored in case of an exception
    o document every rewritten class with an annotation ("@@exception_safe yes")
        -> define rules
    o if no method in the class throws: "@@exception_safe nothrow"
        -> define rules
    o add annotations to *each* file

o refactoring: it would be nicer if the ham_paramter_t lists are passed to
    the Environment implementation and do not have to be parsed in hamsterdb.cc
    (see HAM_PARAM_FILE_SIZE, which is not relevant for in-memory and remote
    Environments)
    - db_config.h: a class with the run-time configuration of
        the database
    - env_config.h: a class with the run-time configuration of
        the environment
    - These classes initialize themselves with the parameters supplied by
        the user, and call all other modules (device, cache...) with this
        structure as the parameter for initialization
    - All getters of these classes are const/immutable

o refactoring: clean up the close() code of the LocalEnvironment (and also
    of the LocalDatabase). distinguish two cases:
    1) a close() called by ham_env_close/ham_db_close
        performs logic that can fail; must not be exception safe
    2) a close() called in the destructor
        do not perform any logic besides cleaning up; must NOT THROW!

o refactoring: get rid of the virtual inheritance for hola workers

o refactoring: when adding more KeyLists/RecordLists (i.e. with compression)
    then we end up in troubles because of code inflation, and because
    the BtreeIndexFactory will become so complex.
    check if it's possible to unify the KeyList/RecordList; both are fairly
    identical with one exception: the RecordLists have to support duplicates
    (but i think this should be manageable)
    -> yes, it's possible. The KeyList has a few methods that are not
        required by the RecordList, and vice versa
    -> try to get rid of these methods; if not possible then create a common
        base class and derive the KeyList and RecordList from it
    -> then replace
            InlineRecordList -> PodKeyList<uint64>
            InlineRecordList -> BinaryKeyList
            (other will follow as soon as compression is enabled)

o refactoring: rewrite capacity calculations (again...)
    -> in LocalDatabase::create -> call calc_estimated_capacity()
    -> in initialize() of the node implementation
    -> in requires_split()
    -> in adjust_capacity()
    o come up with a universal solution that works in all cases, and where
        the logic is implemented in the node/KL/RL, and nowhere else!
      - initialize(), calc_estimated_capacity():
        -> from each KL/RL collect
            - static overhead for metadata
            - estimated size for each item
      - requires_split(): simply check if another item *could be inserted*
      - adjust_capacity(): KL/RL should know whether to grow or shrink
      - some KLs (Bitmap!) require a minimum range size
    o should requires_split() and insert() be merged? and always throw an
        exception (LIMITS_REACHED) if the insert would fail, and rearranging
        would fail?
        -> yes! Also, the function could inform how much *additional space* the
            list requires; this would make the resize function much simpler!
    o rename copy_to() to remove_and_append()

o refactoring: once more check if we can unify the PaxLayout and
    the DefaultLayout

o refactoring: Wouldn't it be nice if the erase/insert routines are
    performed without checking sizes, and if they discover that a
    split (or merge) is required they simply throw; the caller asks the
    janitor to perform this action and then retries? if the code is
    exception safe then there shouldn't be any issues.

o The C# PInvoke for ham_cursor_find is missing the Record structure
    - also accept flags in ham_db_find and ham_cursor_find for approx.
        matching, and copy the key!

o erlang: 2 tests are failing
    o quickcheck-ci.com
    o look into Thomas' mail

o improve performance for hola-sum (profile!)
    http://sites.utexas.edu/jdm4372/2010/11/09/optimizing-amd-opteron-memory-bandwidth-part-3-single-thread-read-only/
    s1 = a1 + b1
    s2 = a2 + b2
    s3 = a3 + b3
    s4 = a4 + b4
    return s1 + s2 + s3 + s4

o refactoring: improve the code separation of cursor and txn_cursor, i.e.
    in db_local::cursor_get_record_size (wtf - is_null(0)?)
    etc

o QuickCheck: create a new property for testing duplicates; the key is always
    the same. The number of duplicate keys is tracked and periodically
    checked with the API. A cursor can be used to remove a specific
    duplicate, or to fetch a specific duplicate.

o QuickCheck: create a new property for testing recnos, with lots of deletes

o windows cleanups
    o need win32 python project (or wait till someone needs it?)
    o a few projects are not built (hamzilla_debug_x64, server_dll_x64...)
        or end up in wrong directories
    o C# unittests sporadically fail b/c a handle is closed multiple times
    o automate the build process
    o ... and the packaging

o delta updates managed in the BtreeNode
    the operations are attached to the node, but as soon as the node
    is accessed for a lookup or a scan, or immediately before the node
    is flushed, the deltas are merged into the node. So far this does not
    sound too efficient, but the bulk updates in combination with the
    key compression (i.e. for prefix compression) will benefit a lot.

    Also, they are really required for concurrency and to allow multiple
    writers in parallel.

    x perform a benchmark/profiling: random inserts vs. ascending inserts;
        the difference should be caused by memcopy/memmove (is it?)
        x PAX
            -> absolutely worth the effort, about 60% are spent in memmove
        x Default
            -> also worth the effort, about 15% are spent in memmove

    o need a flag to disable DeltaUpdates
        o add flag to ham_bench
    o rename TransactionOperation to DeltaUpdate, decouple code from txn
    o totally transparent to the caller, handled in the proxy
    o only add deltas to leaf nodes; internal nodes have too many read
        operations and would anyway require immediate flushes
    o DeltaUpdate objects from a txn-flush should directly go down to
        the node (detach from txn, attach to node)
    o should the the insert + set_record operations be combined into a
        single call? this has the additional advantage that the overhead
        of calling set_record will disappear
    o merge delta updates when reading and flushing
        o however, for simple lookup calls (no duplicates) the DUs can
            be traversed, too 
    o requires_split() must take delta updates into account
    o make the merge algorithm as efficient as possible
        o sort deltas by key
        o first execute all 'erase' either against other deltas or against
            the node
        o then merge the remaining inserts
        o this needs lots of tests
    o now run tests: should every update be stored as a DeltaUpdate? If not
        then disable them by default, unless Transactions are used (and unless
        bulk updates are used)

. documentation rewrite
    o use github wiki
        o remove old cruft

    /introduction
    /evaluate & benchmark
    /faq (merge w/ performance)
    /tutorial
    /pro
    /c, c++
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (one file per function)
        samples
    /java
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (one file per function)
        samples
    /dotnet
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (one file per function)
        samples
    /erlang
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (one file per function)
        samples
    /python
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (one file per function)
        samples
    o look for a technical writer to review the files

. use cache-oblivious b-tree layout
    ===> a much simpler approach would be to reserve "X" bytes when creating
        the environment (and then map the whole range)
        this would include storage for the blobs AND we would not have to mess
        around with the mapping
    -> http://supertech.csail.mit.edu/cacheObliviousBTree.html
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
        (but what about the blobs??)
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    o after resize: mmap the whole file area. this is actually important because
        mmap is much faster than r/w; but when the database is created, the
        original mapping already exists. therefore we might have to handle
        more than one mapping in the file
    o PageManager: when allocating a new page then use the distribution
        function to fetch a page from the reserved storage
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)
        -> could create a second memory mapping for the next chunk

. improve node layout compression (when the file format changes)
    -> btw: flags are not required at all if fixed length record size > 8
        (and record size is too large for inline records)
    o PAX Layout: can compress the flags for the DefaultRecordList
    o Default Layout: can compress the flags for the Duplicate*RecordList

. release-v2.pl: valgrind takes sooo long. should we use clang's
    AddressSanitizer instead? or both?

. hola - next steps
    o support java api
    o support .net api
    o support erlang api
    o lua-functions as callbacks - then remote marshalling will work
    o PRO: compile callbacks with clang remotely
    o add remote support where it makes sense (only for PRO?)

. architecture for a new webpage
    o pick an awesome design
        i.e. similar to http://foundationdb.com, http://laravel.com,
        http://rethinkdb.com, http://www.orientechnologies.com
    o use make/m4/markdown to generate static pages:
        https://github.com/datagrok/m4-bakery
        https://developer.github.com/v3/markdown/
    o come up with the full site structure/contents
        o include full documentation, one page per API
        o ... and for all languages
        o keep the documentation in the source tree, not in -www?
    o documentation comments are hosted on disqus
    o blog comments are hosted on disqus, articles are also written in markup

    o static pages will be served with nginx, using url rewrites
    o dynamic pages are created with a micro-framework (silex?)
    o Makefile can "scp -r" everything to the servers (staging or production)

    . client area with (low priority)
        o authentication
        o collection of files
        o analytics (who downloads what and when?)
    . admin area with (even lower priority)
        o authentication
        o customer database
        o implementing business processes
        o sending out release emails
        o importing new releases
        o etc

. look into spark, mongodb and mariadb integration
    -> would be an interesting market with requirements for column store
        storage
    -> how does the integration look like?
    -> has TPC-benchmarks and other benchmarks ready
    o write down everything i find, collect information and estimate
        efforts/gains (will do the same for mysql, R and mongodb later); this
        task is not about writing code!

------------- hamsterdb pro 2.1.9 ------------------------------------

o PRO: rebase to vanilla 2.1.9

o PRO: use grouped varints for the RecordLists
    -> use in combination with "distance encoding" - only encode the distances
        between the values! i.e.
        <start><flags><d1|d2|d3|...>
          d2 is the difference of d2 to d1, d1 is the difference of d1 to start
          the "d"-values are compressed. usually they are very small and
          therefore require little space.
    o can also be used for PodKeyList<uint32> and PodKeyList<uint64>?
        -> yes, absolutely
    o should work for InternalRecordList (64bit record IDs)
        record ID could be modulo page-size (makes numbers smaller)
    o also for DefaultRecordList, if flags are separated from the record id's
    o and for default records
        o in this case the record flags should also be "compressed"
    o GroupedVarints should be parameterized and work for 32bit and 64bit
        numbers and a variety of capacities
    https://github.com/stuhood/gvi/blob/master/src/main/java/net/hoodidge/gvi/GroupVarInt.java
    http://www.oschina.net/code/snippet_12_5083
    http://www.ir.uwaterloo.ca/book/addenda-06-index-compression.html
    -> the concept will be similar to prefix compression
    -> can we come up with a schema that allows compression AND
        fast random access?

o PRO: bulk updates
    - require delta updates
    - give users API to allocate memory for keys/records
    - if user says that data is already sorted then do not re-sort, but
        fail hard if the sort order is violated
    - add those delta updates to the txn-trees or to the btree node,
        simply transfer ownership of the memory
    -> or are these "batched" updates, in combination with cache-oblivious
        btrees? the batched updates contain lists of structures with
        information about the update (i.e. insert, erase etc). internally,
        a cursor is used to perform the update. this would be fast and a
        relatively cheap way to perform multiple operations in one
        single transaction. and they would not require delta updates, but
        still provide real value.
    o also for remote, transfer in a single message
    o also for .NET
    o also for java
    o also for erlang
    o also for python

o PRO: prefix compression for variable-length keys
    ==> not efficient for random read/writes, but that's ok. linear scans
        and appends will nevertheless be fast, and the delta updates
        will make it efficient too 
    ==> when performing bulk updates/batched updates, make sure that the
        page is compressed only once
    ==> only for leaf keys!? internal keys have too much "distance", and
        the decompression would require too much time
    ==> implement this as an "aspect" for the default-layout with
        variable-sized keys

    o if key is appended at the end: just write the delta
    o every "n'th" (50th?) key is written uncompressed
    o otherwise append a delta-update to the page, and "merge" all deltas
        before the page is flushed. This will improve performance for batched
        updates, since they will cause only one page compression for
        multiple updates.
    o however, it's tricky to figure out whether a node requires a split or
        not, since it requires a good estimate of the size for the new key 
        -> if in doubt then just perform the merge
    
    o full keys can be further compressed with lzf or lzo
    o key search: jumps from full key to full key; in between, there's a
        linear search for the key

------------- hamsterdb 2.1.9 ---------------------------------------

. hola: use sum-prefix-trees to precalculate partial sums/results?
    they could be stored in a btree, and used to dynamically recalculate
    requested values 
    https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o the bucket for concurrency TODOs
    this are the requirements:
    - PRO: inserts are fully concurrent (APL: only one insert at a time)
    - transactions are flushed in background
    - dirty pages are flushed in background
    - reading pages is synchronous, writing pages is asynchronous
        -> needs a very fast locking scheme
    - SSDs support many async. writes in parallel - be prepared!
    http://meetingcpp.com/tl_files/2013/talks/scaling_with_cpp11_edouard_alligand.pdf

    o an intermediate stage would be to have concurrent reads but exclusive
        writes (one writer, many readers) - makes sense?

    o create coding rules
        o synchronization either in lowest or highest level of the callstack
        o when to use what kind of synchronization
        o which modules are single-threaded/synchronized, which are atomic,
            which are concurrent?

    o how can we verify the correctness of the implementation?
        - i.e. by using static analysis or dynamic analysis, together
            with annotated code/custom tools? even if these tools do not
            exist then a system of annotations/naming conventions can help
            writing/maintaining the code

    o mutex implementations should have built-in monitoring, i.e.
        number of locks/unlocks, wait times, contention etc
        o need read/write mutex, fast latches

    o come up with a list of all functions, define which locking operation
        is required; then review the code and make sure this will work
        o the environment configuration
        o the database configuration
        o the transaction tree handling
        o the page manager, the device and the cache
        o the btree
        o the btree nodes (i.e. extkeycache, compressor)
        o parallel lookups (using the same memory arena)

    o reduce the linked lists - they're hard to be updated with atomic
        operations
        o page
        o transaction and dependent objects
        o ...

    o separate SMOs from the actual operation (#2)
        -> check the literature
        http://pdf.aminer.org/000/409/763/b_trees_with_relaxed_balance.pdf
        o move SMO operations to "the janitor" (btree_janitor.h)

    o the global environment-lock should go because it's expensive; rather
        increment an atomic latch, and refuse to close/erase the database as
        long as the latch is > 0 




o PRO: hot backups (vacuumizes to a different file)
    really only for PRO?
    http://sqlite.org/c3ref/backup_finish.html
    - make sure that all transactions are closed
    - perform ham_env_flush
    - then copy the file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    --> think this through; how to deal with delta updates? -> merge them
        what if only a few databases should be backed up?
        what if i want to back up in a logical format (i.e. csv)?

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    -> see supersonic:
        https://code.google.com/p/supersonic/
    -> see fast bitmap indices
        http://code.google.com/p/lemurbitmapindex/
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?
    o use async operations or futures/promises
    o deprecate ham_db_get_key_count() (tutorial, documentation)

- bloom filter -> PRO
- concurrency -> PRO

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

. PRO: should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvice, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

