I Am Legend:

Items are sorted by priority (highest on top).
o a pending  TODO item (for the current release)
. a pending  TODO item (for future releases)
x a finished TODO item

==============================================================================

------------- hamsterdb-erlang 2.1.9 ---------------------------------

x upgrade to hamsterdb-2.1.9

o improve erlang integration
    o integration for quickcheck-ci.com
    x look into Thomas' mail

------------- hamsterdb pro 2.1.9 ------------------------------------

x PRO: review AES usage in read_page; if the page is mmapped then decryption
    is not applied - no, it's ok. encryption disables mmap

x PRO: rebase to vanilla 2.1.9

x PRO: refactoring: make AES Encryption stateless and exception safe
    (if possible)

x PRO: refactoring: make Compressors stateless and exception safe
    (and maybe w/o inheritance?)

x PRO: review SIMD code; the problem is that it's never called, because the
    search threshold is always smaller than the minimum block size
    x use fixed block size, otherwise fallback to non-simd search
    x run performance tests - ok

x PRO: Can we release an early version of zint32? (undocumented)
    x need to fix erase() handling
    x add test coverage in unittests
        x insert w/ appends, lookup, erase, lookup
        x insert w/ prepends, lookup, erase, lookup
        x insert w/ random, lookup, erase, lookup
    x add full test coverage in monster tests

------------- hamsterdb 2.1.10 ---------------------------------------

x documentation rewrite
    x use github wiki, remove old cruft
    x then create the initial repository:
        https://github.com/blog/699-making-github-more-open-git-backed-wikis

    x /introduction
    x /evaluate & benchmark
    x /faq (merge w/ performance)
    x /tutorial
        ... (existing stuff)
    x /pro
    x /hola
    x API pages
        overview (description, status)
        installation
        usage (compiling, linking)
        api functions (link to doxygen)
        samples
        x c
        x c++
        x java
        x dotnet
        x erlang
        x python
        x protobuf

    x have the hamsterdb/documentation directory "link" to the git project

x get rid of HAM_API_REVISION, move HAM_VERSION_* to the header file, or
            move version.h to include/ham

x refactoring: the ByteArray should be non-copyable, or at least
    clearly define the copying semantics regarding ownership. Also
    review its uses, try to figure out how it can be improved. Should it be a
    DynamicArray<uint8_t>?

x Zint32: still has a few bugs when running unittests because
    index->used_size is miscalculated
    gdb --args ./test Zint*
    b zint32.cpp:66
    b hamsterdb::Zint32::Zint32KeyList::copy_to
    x then once more check the monster tests

x remove dependency to malloc.h, use stdlib.h instead

x add patches from Thomas Fähnle

x refactoring: clean up the whole Database code; it's too much and too
    complex; there must be an absolutely clear border between Btree and
    Transaction related code.
    -> should we move the transactional implementations to their own
        database class, i.e. TransactionalDatabase?
    x LocalDatabase::cursor_get_duplicate_position: delegate to Cursor class
    x when to call cursor->set_lastop(Cursor::kLookupOrInsert)?
    x LocalDatabase::cursor_insert -> calls LocalDatabase::insert
        x remove BtreeCursor::insert, it does not have relevant code
    x LocalDatabase::cursor_erase -> calls LocalDatabase::erase
        x remove BtreeCursor::erase
        x remove TransactionCursor::erase
    x LocalDatabase::cursor_find -> calls LocalDatabase::find
        x update BtreeCursor::find
    x a lot of functions have to clear the changeset when they leave;
        use ON_EXIT_SCOPE macro for this!
    x run performance tests
    x run monster tests

x #42: approx. matching: cursor returns wrong key when using transactions
    x create a unittest to reproduce this
    x the fix should be that ham_cursor_find calls ham_db_find(..., cursor)

x cherry-pick the following commits on the master branch
    1447ba4eb217532e8fb49c4a84a0dc3b982a3ffe
    6a8dd20ec9bd2ec718d1136db7667e0e58911003

x Thomas Fähnle: create new parameter for fadvise configuration
    x implement for file-based (w and w/o memory mapping) devices 
    x need to check for the function in configure.ac
    x return flag with ham_env_get_parameter (not documented)
    x create a unittest to make sure that the parameter is not persisted
    x create a new parameter for ham_bench

x A few things to refactor in the btree
    x the BtreeAction classes do not need a Transaction pointer as an argument!
    x actually NO part of the Btree should EVER access a Transaction...
    x btree_visitor.h has a structure called ScanVisitor, and BtreeVisitor is
        in btree_index.h. Can both be combined?

x Introduce new option for record numbers (HAM_RECORD_NUMBER32,
    HAM_RECORD_NUMBER64), deprecate the old one. Use 32bit whenever possible!
    x introduce a new flag; the old one uses 64bit. all tests must work
    x implement (local)
    x implement (server)
    x implement (remote)
    x verify with ham_dump/ham_info
    x add unittests
        x re-use existing tests
        x also for remote!
        x check for overflows!
    x add to ham_bench
    x fix documentation
        x samples
        x header file

x implement Cursor::get_record_size for remote access

o Zint32: add compression based on bit-packing
    -> https://github.com/lemire/simdcomp/blob/master/include/simdintegratedbitpacking.h
    x add parameter to header file
    x support in ham_bench
    x update IndexFactory for the new KeyList
    x move block/index related functionality to a base class BlockKeyList
        (w/ template parameter for the block structure)
    x integrate Daniel's sources as a library
        x add a unittest to compress/decompress
    x implement the compression
        x don't use index->used_size, it's not required
        x don't use index->block_size; it's always static
        x use delta-encoding to have better compression
        x fill in the implementation
        x add unittests (similar to zint32)
    x add monster tests (similar to zint32/varbyte)
    x don't calculate bitwidth if index->bits is already maxxed out (32)
    x there is no vacuumize prior to a split. The vacuumize is cancelled
        immediately because it is always "internal".
    x don't split blocks in the middle if the new key is appended
    x don't split blocks in the middle if the new key is prepended
    o test with bulk-erase and deltas of size 1, no records
        -> no delete stability!
    o fix the remaining TODOs
        o improve copy_to() performance by allocating multiple
            blocks at once
        o improve vacuumize_impl() performance by allocating multiple
            blocks at once
        o add functions for simdcomp library to deal with underfilled blocks

x Remove requirement for "delete stability" when erasing keys
    x Introduce generic BtreeUpdateAction which handles merges and splits
    x when erasing a key: also expect splits; in case of a split the
        BtreeEraseAction has to retry the operation - either with the old
        or the new node!

o hamsterdb-erlang: add support for HAM_RECORD_NUMBER32

o Concurrency: move "purge cache" to background
    1) page list is not modified while it is traversed/modified
        - the purger will traverse the list only from back to front
        - the list's operation are only atomic regarding back to front
            traversals, not reverse
        - the purger must not modify the list; it will only unmap/free the
            page buffer!
    2) it must be an atomic operation to lock the page as "in use" and to
        check this lock!
    3) PageManager: when fetching a page that is currently flushed to disk:
        wait till flush is finished, then return the page
    - Alternatively, reorg the hash table. All buckets are in serial memory.
        This requires a few allocations, but reduces cache misses because
        there's no "jumping" from page to page. Such a bucket can be replaced
        atomically with CAS (hazard pointer!) and appended w/o locking. And
        it avoids frequent calls to remove_from_list/insert_to_list to push
        the used pages to the front. As a consequence, a new purge mechanism
        is required to identify "old" pages. These pages then could even be
        sorted before they are flushed.
    - Try to get rid of the page lists. Have a generic "PageCollection<Max>"
        class with statically allocated storage for Max pages, and which uses a
        pointer for more pages if Max is not sufficient. Use this in the
        Changeset and the Cache buckets.
    o add a flag to disable background operations
    o get rid of the Page-List (use a generic visitor for the Cache)
    o rewrite all other lists with the PageCollection
        o should allow reading w/o blocking
        o appending pages should be "free" (no lock, unless the list must
            grow)
        o to delete entries just overwrite them with zeroes
        o use CAS/hazard ptr to allocate and append new lists
        o there will be multiple writers and multiple readers
        - think this through; CAS and appends are not necessarily atomic!
          also, if elements are deleted with 0 then it might make sense to
          reorganize those lists if there are too many deleted elements
    o create a background thread; use libuv
    o purger thread periodically walks over the list, purges
        o can be woken up by the main thread
    o if a page is deleted or closed: let purger thread clean up the pages
    o page must not be purged while it's "in use"
    o mark page as "in use" during purge
    o run performance tests
    o and run the leveldb benchmarks

o check recovery.pl - extended_tests are failing on PRO because the
    error inducer hits too early

o migrate to libuv 1.0, it has a stable API
    http://docs.libuv.org/en/latest/migration_010_100.html
    o also for windows!

o refactoring: make EnvironmentHeader stateless and exception safe
    -> will it ever be used outside of the EnvironmentConfiguration setting?
    -> can we remove it?

o support multiple worker threads which perform I/O
    o how many threads should be created??

o documentation related issues for 2.1.10
    o HAM_RECORD_NUMBER32: add/update tutorial

o reserve file size upon creation, then map the whole range
    o needs new parameter HAM_PARAM_INITIAL_FILE_SIZE
    o automatically page-align the size
    o only for disk-based devices!
        o unittest
    o the device will manage the mapping
        o unittest - verify that the initial file size is correct
        o unittest - verify that the file size does not grow if new pages
            are allocated
    o support in ham_bench
    o monster tests and performance tests
    o also for java
    o also for dotnet
    o also for python
    o also for erlang

o If mmap is enabled then keys and records don't have to be copied as long
    as they point into the mapped area!
    - If a key is in a mapped page (w/o extended key) then simply return a
        pointer
    - If a record is in a mapped page then simply return a
        pointer

o More things to refactor in the btree
    o PageManager::fetch_page should be available in a const version
        (fetch_const_page) which sets a flag in the page ("kIsImmutable").
        the NodeProxy must test this flag whenever it modifies a page!
        (debug only)
    o EraseAction uses duplicate_index + 1, InsertAction uses duplicate_index
        -> use a common behaviour/indexing
    o EraseAction line 71: if the node is empty then it should be merged and
        moved to the freelist!

o when splitting and HAM_HINT_APPEND is set, the new page is appended.
    do the same for prepend!







o refactor LocalDatabase::cursor_insert: when done, the duplicate index is
    updated. In most cases (DUPLICATE_LAST), a duplicate table has to be built,
    but it's unclear whether the duplicate index is ever required. Better
    use a logical index ("kLastDuplicate") and lazily calculate the actual
    position when it's required.

o refactoring: db_local.cc has so many TODOs!

o refactoring: improve the code separation of cursor, btree_cursor
    and txn_cursor, i.e. in db_local::cursor_get_record_size (wtf - is_null(0)?)
    etc
    o the whole cursor state is messed up. there should be 3 states:
        - nil
        - coupled to btree
        - coupled to txn
        and nothing else!
    o there should be a get_key() and a get_record() method; the caller should
        not get access to txn_cursor/btree_cursor etc
    o cursor.cc has so many TODOs!
    o review if the Cursor class has access to any shared resource; what
        about the cursor list??

o continue with concurrency...
    next stage: flush changesets in background. this should work whenever
    recovery is enabled (with and without transactions)!

o improve the webpage documentation
    o document the various btree formats on the webpage, with images
        o variable length keys (w/ extended keys)
        o POD keys
        o default records
        o inline records
        o fixed-length records
        o duplicates (w/ overflow tables)
        o PRO: compressed keys
        o PRO: compressed records

o internal nodes should use 32bit page IDs!






o PRO: prefix compression for variable-length keys
    use an indirection for the prefixes and suffixes; store each
    part in a slot. the keys themselves have then fixed length (2 slot id's)
        ==> supports efficient binary search!
        ==> is efficient for random read/writes AND linear scans
        however, it's very complex to figure out how to optimally split the
        strings into prefixes and suffixes
    ==> prefixes and suffixes can be stored as extended keys if they become
        too large
    see indexcompression2009.pdf - Efficient index compression in DB2 LUW
    o look for more research papers


o PRO: allow compression of 32bit record numbers

o PRO: look for a better compression for DefaultRecordList, i.e.
    - Each group is a GroupedVarInt w/ 4 bits per entry; a 64bit
        number can then hold flags for 16 numbers
        -> (but maybe increase this to hold at least 32 or 64 numbers, to
            reduce the overhead ratio)
    o create a GroupedVarInt<Max, T> class, where |Max| is the maximum number
        of elements that are grouped, and T is the type of these elements
        (i.e. uint64_t)
        -> memory is managed by the caller
        -> the state (i.e. used block size etc) is stored externally, and
            managed by the caller
        o append a key
        o prepend a key
        o insert a key in the middle
        o grow blocks
        o split blocks
        o can perform copies w/o re-compressing

    o try to move the Zint32 index to a base class
    o Use small index which stores offset + bits for each group
    o a separate bit is used to signal whether the (compressed) number is
        a record id
    o avoid ripple effects by growing/splitting the block

o PRO: Use the GroupedVarInt also for duplicate groups

o PRO: find an efficient compression method for InternalRecordList
    -> also, this should play well with the compression of the DefaultRecordList
    x The page IDs should be modulo page-size (makes numbers smaller) - done

o PRO: use compression also for duplicate records

. release-v2.pl: valgrind takes sooo long. should we use clang's
    AddressSanitizer instead? or both?

. hola - next steps
    o support java api
    o support .net api
    o support erlang api
    o lua-functions as callbacks - then remote marshalling will work
    o PRO: compile callbacks with clang remotely
    o add remote support where it makes sense (only for PRO?)

. architecture for a new webpage
    o pick an awesome design
        i.e. similar to http://foundationdb.com, http://laravel.com,
        http://rethinkdb.com, http://www.orientechnologies.com
    o use make/m4/markdown to generate static pages:
        https://github.com/datagrok/m4-bakery
        https://developer.github.com/v3/markdown/
    o come up with the full site structure/contents
        http://sidekiq.org/pro/
        o include full documentation, one page per API
        o ... and for all languages
        o keep the documentation in the source tree, not in -www?
    o documentation comments are hosted on disqus
    o blog comments are hosted on disqus, articles are also written in markup

    o Makefile can "scp -r" everything to the servers (staging or production)

    . client area with (low priority)
        o authentication
        o collection of files
        o analytics (who downloads what and when?)
    . admin area with (even lower priority)
        o authentication
        o customer database
        o implementing business processes
        o sending out release emails
        o importing new releases
        o etc

. hola: use sum-prefix-trees to precalculate partial sums/results?
    they could be stored in a btree, and used to dynamically recalculate
    requested values 
    https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf

o QuickCheck: automatically test the recovery feature by invoking "crashes"

o QuickCheck: create a new property for testing duplicates; the key is
    always the same. The number of duplicate keys is tracked and
    periodically checked with the API. A cursor can be used to remove a
    specific duplicate, or to fetch a specific duplicate.

. use cache-oblivious b-tree layout
    -> http://supertech.csail.mit.edu/cacheObliviousBTree.html
    o see roadmap document for more information
    o this feature is *per database*
    o calculate number of reqd pages based on estimated keys from the user
        (but what about the blobs??)
    o make sure that this is not reverted when "reduce file size" feature
        (above) is enabled
    o the new pages are not managed by the freelist! therefore the freelist
        will not need any modifications
    o after resize: mmap the whole file area. this is actually important because
        mmap is much faster than r/w; but when the database is created, the
        original mapping already exists. therefore we might have to handle
        more than one mapping in the file
    o PageManager: when allocating a new page then use the distribution
        function to fetch a page from the reserved storage
    . try to batch allocations; when new pages are required then don't just
        allocate one but multiple pages (if the database is big enough)
        -> could create a second memory mapping for the next chunk

o refactoring: create a Update structure whenever a database is modified;
    it consists of the operation type, key, record (optional), operation flags,
    ByteArray for storing the result(s) etc. Can reuse the TransactionOperation
    class.
    This structure is then forwarded to the journal, and to all BtreeActions.
    The BtreeActions no longer require state (at least large parts of it).
    This structure will also be used for the DeltaUpdates and for batched
    updates later on.
    o insert: create such a structure in ham_db_insert, forward it to the
        lower layers
    o erase: create such a structure in ham_db_erase, forward it to the
        lower layers
    o same for cursor functions
    o same for ham_cursor_overwrite
    o can we get rid of Database::cursor_insert, cursor_find, cursor_erase?

o delta updates managed in the BtreeNode
    the operations are attached to the node, but as soon as the node
    is accessed for a lookup or a scan, or immediately before the node
    is flushed, the deltas are merged into the node. So far this does not
    sound too efficient, but the bulk updates in combination with the
    key compression (i.e. for prefix compression) will benefit a lot.

    Also, they are really required for concurrency and to allow multiple
    writers in parallel.

    x perform a benchmark/profiling: random inserts vs. ascending inserts;
        the difference should be caused by memcopy/memmove (is it?)
        x PAX
            -> absolutely worth the effort, about 60% are spent in memmove
        x Default
            -> also worth the effort, about 15% are spent in memmove

    o need a flag to disable DeltaUpdates
        o add flag to ham_bench
    o rename TransactionOperation to DeltaUpdate, decouple code from txn
    o totally transparent to the caller, handled in the proxy
    o only add deltas to leaf nodes; internal nodes have too many read
        operations and would anyway require immediate flushes
    o DeltaUpdate objects from a txn-flush should directly go down to
        the node (detach from txn, attach to node)
    o should the the insert + set_record operations be combined into a
        single call? this has the additional advantage that the overhead
        of calling set_record will disappear
    o merge delta updates when reading and flushing
        o however, for simple lookup calls (no duplicates) the DUs can
            be traversed, too 
    o requires_split() must take delta updates into account
    o make the merge algorithm as efficient as possible
        o sort deltas by key
        o first execute all 'erase' either against other deltas or against
            the node
        o then merge the remaining inserts
        o this needs lots of tests
    o now run tests: should every update be stored as a DeltaUpdate? If not
        then disable them by default, unless Transactions are used (and unless
        bulk updates are used)

o PRO: hot backups (vacuumizes to a different file)
    really only for PRO?
    http://sqlite.org/c3ref/backup_finish.html
    - make sure that all transactions are closed
    - perform ham_env_flush
    - then copy the file
    - if compaction is enabled: copies keys w/ iterator
        (later: performs bulk updates)
    --> think this through; how to deal with delta updates? -> merge them
        what if only a few databases should be backed up?
        what if i want to back up in a logical format (i.e. csv)?

o "hola" - olap functions that operate directly on the btree data
    -> see wiki
    -> see java8 stream API:
        http://download.java.net/jdk8/docs/api/java/util/stream/Stream.html
    -> see supersonic:
        https://code.google.com/p/supersonic/
    -> see fast bitmap indices
        http://code.google.com/p/lemurbitmapindex/
    o create a design
    o operations on compressed data (COUNT(), MIN(), MAX(), ...)?
    o use async operations or futures/promises
    o deprecate ham_db_get_key_count() (tutorial, documentation)

- bloom filter -> PRO
- concurrency -> PRO

. clean up approx. matching
    o ONLY for cursors
    o Flags: HAM_FIND_LT_MATCH | HAM_FIND_GT_MATCH | HAM_FIND_EQ_MATCH (default)
    o lookup: the cursor is coupled to the key, even if the lookup fails
        then perform a lookup:
            found_key == requested_key:
                HAM_FIND_EQ_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_GT_MATCH: return move_next()
            found_key < requested_key:
                HAM_FIND_LT_MATCH: ok
                HAM_FIND_GT_MATCH: return move_next()
                HAM_FIND_EQ_MATCH: key not found
            found_key > requested_key:
                HAM_FIND_GT_MATCH: ok
                HAM_FIND_LT_MATCH: return move_prev()
                HAM_FIND_EQ_MATCH: key not found
    o must work with transactions
    o do not store key flags; the caller has to compare the key
    o remove ham_key_set_intflags, ham_key_get_intflags, key->_flags (?)

. win32: need a release-v2.pl which fully automates the various release steps
    o delete all generated protobuf files
    o build for msvc 2008
    o run unittests for debug and release
    o run samples
    o delete all generated protobuf files
    o build for msvc 2010
    o run unittests for debug and release
    o run samples
    o build release package

. also remove locking from C# and Java APIs

------------------- idea soup ---------------------------------------------

. PRO: should we have a separate "recsize == 0" RecordList for duplicates?
    they could only store the duplicate count (but should be able to deal
    with duplicates that are > 256!)
    -> requires grouped varints

o asynchronous prefetching of pages
    -> see posix_fadvise, libprefetch

o when recovering, give users the choice if active transactions should be
    aborted (default behavior) or re-created
    o needs a function to enumerate them

o A new transactional mode: read-only transactions can run "in the past" - only
    on committed transactions. therefore they avoid conflicts and will always
    succeed.

o need a function to get the txn of a conflict (same as in v2)
    ham_status_t ham_txn_get_conflicting_txn(ham_txn_t *txn, ham_txn_t **other);
        oder: txn-id zurückgeben? sonst gibt's ne race condition wenn ein anderer
        thread "other" committed/aborted
    o also add to c++ API
    o add documentation (header file)
    o add documentation (wiki)

. new test case for cursors
    insert (1, a)
    insert (1, b) (duplicate of 1)
    move (last) (-> 1, b)
    insert (1, c)
    move (last) (-> 1, c)? is the dupecache updated correctly?

. there are a couple of areas where a btree cursor is uncoupled, just to
    retrieve the key and to couple the txn-key. that's not efficient
        db.c:__btree_cursor_points_to
        db.c:__compare_cursors
        txn_cursor.c:cursor_sync
        txn_cursor.c:cursor_overwrite
    o move to a separate function
    o try to optimize

. add tests to verify that the cursor is not modified if an operation fails!
    (in cursor.cpp:LongTxnCursorTest are some wrapper functions to move or
    insert the cursor; that's a good starting point)

